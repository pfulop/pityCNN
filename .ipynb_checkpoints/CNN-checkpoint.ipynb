{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn import learn_runner\n",
    "import pandas as pd\n",
    "from os import path\n",
    "from inputs import convert\n",
    "from tensorflow.contrib.learn import ModeKeys\n",
    "from tensorflow.contrib.data import Iterator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block(inputs, filters, name):\n",
    "    conv1 = tf.layers.conv2d(inputs,\n",
    "                            filters,\n",
    "                            4,\n",
    "                            padding=\"same\",\n",
    "                            activation=tf.nn.relu,\n",
    "                            name=\"conv{}_1\".format(name))\n",
    "    conv2 = tf.layers.conv2d(conv1,\n",
    "                            filters,\n",
    "                            4,\n",
    "                            padding=\"same\",\n",
    "                            activation=tf.nn.relu,\n",
    "                            name=\"conv{}_2\".format(name))\n",
    "    conv3 = tf.layers.conv2d(conv2,\n",
    "                            filters,\n",
    "                            4,\n",
    "                            padding=\"same\",\n",
    "                            activation=tf.nn.relu,\n",
    "                            name=\"conv{}_3\".format(name))\n",
    "    pool = tf.layers.max_pooling2d(conv3,\n",
    "                            pool_size=2,\n",
    "                            strides=2,\n",
    "                            name=\"pool{}\".format(name))\n",
    "    if filters == 64:\n",
    "        ix = 260\n",
    "        iy=260\n",
    "        channels=64\n",
    "        V = tf.slice(conv3,(0,0,0,0),(1,-1,-1,-1))\n",
    "        V = tf.reshape(V,(iy,ix,channels))\n",
    "        ix += 4\n",
    "        iy += 4\n",
    "        V = tf.image.resize_image_with_crop_or_pad(V, iy, ix)\n",
    "        cy = 8\n",
    "        cx = 8\n",
    "        V = tf.reshape(V,(iy,ix,cy,cx)) \n",
    "        V = tf.transpose(V,(2,0,3,1))\n",
    "        V = tf.reshape(V,(1,cy*iy,cx*ix,1))\n",
    "        tf.summary.image(\"image_conv{}_3\".format(name), V)\n",
    "        \n",
    "    return pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def architecture(inputs, filters_size, filter_names, output_size):\n",
    "    with tf.variable_scope('CovNet'):\n",
    "        \n",
    "        inputs = tf.layers.batch_normalization(inputs, name=\"batch_normalization\")\n",
    "        \n",
    "        for i,filter_size in enumerate(filters_size):\n",
    "            with tf.name_scope(\"conv-maxpool-{}s\".format(filter_names[i])):\n",
    "                      inputs = block(inputs, filter_size, filter_names[i])\n",
    "\n",
    "        with tf.name_scope(\"conv-dense\"):\n",
    "            inputs = tf.reshape(inputs, [-1, 8 * 8 * 512])\n",
    "            dense1 = tf.layers.dense(\n",
    "                inputs,\n",
    "                4096,\n",
    "                activation=tf.nn.relu,\n",
    "                name='dense1',\n",
    "            )\n",
    "            dense2 = tf.layers.dense(\n",
    "                dense1,\n",
    "                4096,\n",
    "                activation=tf.nn.relu,\n",
    "                name='dense2',\n",
    "            )\n",
    "            net = tf.layers.dense(\n",
    "                dense2,\n",
    "                output_size,\n",
    "                name='dense3',\n",
    "            )\n",
    "    \n",
    "    print(net)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss, Metrics and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_op_fn(loss, params):\n",
    "    return tf.contrib.layers.optimize_loss(\n",
    "        loss=loss,\n",
    "        global_step=tf.contrib.framework.get_global_step(),\n",
    "        optimizer=tf.train.AdamOptimizer,\n",
    "        learning_rate=params.learning_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_eval_metric_ops(labels, predictions):\n",
    "    return {\n",
    "        'Accuracy': tf.metrics.accuracy(\n",
    "            labels=tf.argmax(labels, 0),\n",
    "            predictions=predictions,\n",
    "            name='accuracy')\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):\n",
    "    filter_size = [64,128,256,512,512]\n",
    "    filter_names = ['64','128','256','512a','512b']\n",
    "    output_size = labels.shape[1]\n",
    "    is_trainig = mode == ModeKeys.TRAIN\n",
    "    logits = architecture(features, filter_size, filter_names, output_size)\n",
    "    predictions = tf.argmax(logits, axis = -1)\n",
    "    loss = None\n",
    "    train_op = None\n",
    "    eval_metric_ops = {}\n",
    "    if mode != ModeKeys.INFER:\n",
    "        loss = tf.losses.softmax_cross_entropy(\n",
    "            onehot_labels=tf.cast(labels, tf.int32),\n",
    "            logits=logits)\n",
    "        train_op = get_train_op_fn(loss, params)\n",
    "        eval_metric_ops = get_eval_metric_ops(labels, predictions)\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions=predictions,\n",
    "        loss=loss,\n",
    "        train_op=train_op,\n",
    "        eval_metric_ops=eval_metric_ops\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimator Iterator and Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_estimator(run_config, params):\n",
    "    return tf.estimator.Estimator(\n",
    "        model_fn=model_fn,  # First-class function\n",
    "        params=params,  # HParams\n",
    "        config=run_config  # RunConfig\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class IteratorInitializerHook(tf.train.SessionRunHook):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(IteratorInitializerHook, self).__init__()\n",
    "        self.iterator_initializer_func = None\n",
    "\n",
    "    def after_create_session(self, session, coord):\n",
    "        self.iterator_initializer_func(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_inputs(batch_size, images_paths, labels):\n",
    "    \n",
    "    iterator_initializer_hook = IteratorInitializerHook()\n",
    "\n",
    "    def train_inputs():\n",
    "        \"\"\"Returns training set as Operations.\n",
    "        Returns:\n",
    "            (features, labels) Operations that iterate over the dataset\n",
    "            on every evaluation\n",
    "        \"\"\"\n",
    "        with tf.name_scope('Training_data'):\n",
    "            dataset, images, labls = convert(images_paths, labels)\n",
    "            iterator = Iterator.from_structure(dataset.output_types,\n",
    "                                   dataset.output_shapes)\n",
    "            next_element = iterator.get_next()\n",
    "\n",
    "            # create two initialization ops to switch between the datasets\n",
    "            training_init_op = iterator.make_initializer(dataset)\n",
    "            iterator_initializer_hook.iterator_initializer_func = lambda sess: sess.run(training_init_op)\n",
    "#             iterator = dataset.make_initializable_iterator()\n",
    "#             next_example, next_label = iterator.get_next()\n",
    "#             # Set runhook to initialize iterator\n",
    "#             iterator_initializer_hook.iterator_initializer_func = \\\n",
    "#                 lambda sess: sess.run(\n",
    "#                     iterator.initializer,\n",
    "#                     feed_dict={images: images,\n",
    "#                                labels: labls})\n",
    "#             # Return batched (features, labels)\n",
    "            return next_element\n",
    "\n",
    "    # Return function and hook\n",
    "    return train_inputs, iterator_initializer_hook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def experiment_fn(run_config, params):\n",
    "    DATA_PATH = '/run/media/backman/yay/dogbreed/'\n",
    "    run_config = run_config.replace(save_checkpoints_steps=params.min_eval_frequency)\n",
    "    estimator = get_estimator(run_config, params)\n",
    "    \n",
    "    train_images, train_labels, valid_images, valid_labels = prepare_data()\n",
    "    train_input_fn, train_input_hook = get_inputs(10, train_images, train_labels)\n",
    "    eval_input_fn, eval_input_hook = get_inputs(10, valid_images, valid_labels)\n",
    "\n",
    "    \n",
    "    experiment = tf.contrib.learn.Experiment(\n",
    "        estimator=estimator,  # Estimator\n",
    "        train_input_fn=train_input_fn,  # First-class function\n",
    "        eval_input_fn=eval_input_fn,  # First-class function\n",
    "        train_steps=params.train_steps,  # Minibatch steps\n",
    "        min_eval_frequency=params.min_eval_frequency,  # Eval frequency\n",
    "        train_monitors=[train_input_hook],  # Hooks for training\n",
    "        eval_hooks=[eval_input_hook],  # Hooks for evaluation\n",
    "        eval_steps=None  # Use evaluation feeder until its empty\n",
    "    )\n",
    "    return experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_experiment(argv=None):\n",
    "    \"\"\"Run the training experiment.\"\"\"\n",
    "    # Define model parameters\n",
    "    params = tf.contrib.training.HParams(\n",
    "        learning_rate=0.002,\n",
    "        n_classes=10,\n",
    "        train_steps=5000,\n",
    "        min_eval_frequency=100\n",
    "    )\n",
    "\n",
    "    # Set the run_config and the directory to save the model and stats\n",
    "    run_config = tf.contrib.learn.RunConfig()\n",
    "    run_config = run_config.replace(model_dir='model')\n",
    "\n",
    "    learn_runner.run(\n",
    "        experiment_fn=experiment_fn,  # First-class function\n",
    "        run_config=run_config,  # RunConfig\n",
    "        schedule=\"train_and_evaluate\",  # What to run\n",
    "        hparams=params  # HParams\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    DATA_HOME = '/Users/fulop/Downloads/dogbreed/'\n",
    "    CSV = DATA_HOME+'labels.csv'\n",
    "    TRAIN_PATH = DATA_HOME + 'sample/train/'\n",
    "    VALID_PATH = DATA_HOME + 'sample/valid/'\n",
    "    dfile = pd.read_csv(DATA_HOME+'labels.csv')\n",
    "    dfile['breed'] = pd.Categorical(dfile['breed'])\n",
    "    dfile['breed'].value_counts().plot(kind='bar')\n",
    "    dfile['breed'] = dfile.breed.cat.codes\n",
    "    dfile['type'] = dfile.apply(lambda f: 'train' if path.isfile(TRAIN_PATH+f['id']+'.jpg') else 'valid' if path.isfile(VALID_PATH+f['id']+'.jpg') else 'none', axis=1)\n",
    "    valid = dfile[dfile['type']=='valid']\n",
    "    train = dfile[dfile['type']=='train']\n",
    "    train_images = train['id'].map(lambda name : \"{}{}.jpg\".format(TRAIN_PATH,name)).values\n",
    "    valid_images = valid['id'].map(lambda name : \"{}{}.jpg\".format(VALID_PATH,name)).values\n",
    "    train_labels = train['breed'].values.astype(np.int32)\n",
    "    valid_labels = valid['breed'].values.astype(np.int32)\n",
    "    \n",
    "    return train_images, train_labels, valid_images, valid_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:RunConfig.uid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1147bf7b8>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': None, '_log_step_count_steps': 100, '_session_config': None, '_save_checkpoints_steps': 100, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': 'model'}\n",
      "WARNING:tensorflow:RunConfig.uid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/monitors.py:269: BaseMonitor.__init__ (from tensorflow.contrib.learn.python.learn.monitors) is deprecated and will be removed after 2016-12-05.\n",
      "Instructions for updating:\n",
      "Monitors are deprecated. Please use tf.train.SessionRunHook.\n",
      "Tensor(\"CovNet/conv-dense/dense3/BiasAdd:0\", shape=(?, 10), dtype=float32)\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into model/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.214135, step = 1\n"
     ]
    }
   ],
   "source": [
    "run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
